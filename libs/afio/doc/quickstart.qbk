[/============================================================================
  Boost.AFIO

  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:quickstart Quick Start]

There are two kinds of likely __boost_afio__ users: (i) those who want to do high performance,
scalable file input/output (ii) those who want to execute closures (self contained
functions, think of them like C style callbacks except they can carry state) with
the rich dependency ordered execution constraint support AFIO implements.

The former ends up bleeding into the latter however, which is why AFIO also exposes
for public usage its internal closure execution engine: for example, forthcoming
__triplegit__ [link afio.design.acid_write_ordering.write_ordering_data often runs 128 or 256 bit hashes on every piece of data entering or
exiting storage]. With AFIO, this is as simple as adding a hash execution closure to every
scatter read and gather write operation __dash__ and AFIO's flexibility lets you easily
choose between that hash being performed before i/o (useful for preloading the L2 cache
if scatter/gather chunks are less than 4Kb each), after i/o (useful if reading 4Kb or
larger chunks as DMAs invalidate CPU caches), or more interestingly ['during] i/o i.e.
in parallel (useful if writing 4Kb or larger chunks).

For this quick start, the question becomes which to start with first: async file i/o
or closures? Starting with either has merits, but we've decided on file i/o first.

In either use case, using __boost_afio__ is centred around the __afio_dispatcher__ abstract base class,
whose underlying platform-specific implementation class is instantiated using the
[link afio.reference.functions.make_async_file_io_dispatcher `make_async_file_io_dispatcher()`]
factory function. Once you have a `std::shared_ptr<async_file_io_dispatcher_base>`,
you simply call its member functions to have the dispatcher schedule asynchronous
work items for you, each returning a __afio_op__.

All AFIO scheduling functions, or the convenience structures they take in, will
take as a (sometimes optional) first parameter their precondition op. Only when that
precondition has finished executing will its dependencies become scheduled for
execution. This makes expanding out multiple ops from a single precondition easy,
but what about reducing multiple ops down to a single precondition? For that, use
__afio_barrier__.

AFIO itself throws almost no exceptions apart from `std::invalid_argument`, though it
can throw fatal exceptions when you're doing something unwise (and make sure to check
the FAQ if you do see a fatal exception, it will explain what you're doing wrong). It
does trap all operating system errors, reporting these as the appropriate `std::system_error`
when code next gathers error state from that operation (usually using `when_all()`). One
can of course also receive exceptions thrown by anything AFIO calls (the STL, Boost etc).
AFIO is thought to be exception safe, except when `std::bad_alloc` is thrown.

[section:async_file_io Asynchronous file i/o]

The asynchronous file i/o implementation consists of a number of member functions in
the __afio_dispatcher__ abstract base class, plus an assortment of convenience
classes and structures. They are fairly obviously named, and do fairly obvious things.

First stop: Hello World in STL iostreams and its exact equivalent in AFIO.

[section:hello_world Hello World, asynchronously!]

We might as well jump straight in: on the left is a traditional STL iostreams
implementation with its equivalent in AFIO using single-shot functions (i.e. a batch
of one) on the right.

[table:hello_world Traditional STL iostreams and AFIO side by side
  [[[readwrite_example_traditional]][[readwrite_example]]]
]

For reference, that's 16 lines of code on the left as compared to 21 lines of
code on the right (three of which could be easily eliminated too). Admittedly the
lines are a lot longer, and maybe less intuitive looking, but programming against
AFIO is certainly compact!

This compactness of expression is not by accident: AFIO's API is intended as a sort
of domain specific embedded language (DSEL) for asynchronous file i/o because it's
hard enough writing power-loss safe sequencing of filing system operations without
having to mess around with boilerplate. AFIO's API lets you see more clearly what
happens in what sequence with a minimum of vertical real estate needed (unlike
Windows NT asynchronous file i/o programming in particular).

As you'll note from the bookmarked lines on the right, generally the first parameter
into any AFIO function is the precondition op (of type __afio_op__) which must complete before the op being
scheduled now is allowed to begin. Precondition ops don't just enforce execution order
however, they also carry a file handle or more accurately, a ['shared future] to a
shared pointer to a __afio_handle__ which by definition will be ready by the time your
dependency is executed. Let's take a very quick look at `async_io_op`:

	struct async_io_op
	{
		async_file_io_dispatcher_base *parent;                              //!< The parent dispatcher
		size_t id;                                                          //!< A unique id for this operation
		std::shared_ptr<shared_future<std::shared_ptr<async_io_handle>>> h; //!< A future handle to the item being operated upon
	};

So, because there can be many op references to an op, `h` is a shared pointer to a unique shared future,
one of which exists per unique integer id. Right now AFIO keeps a pointer to a parent
as well in case future AFIO's can chain ops across dispatcher instances, but for now
it simply tells you which dispatcher the op is or was scheduled with.

The shared future is valid as soon as an op is sent for execution to the thread source (which may not occur
until after any preconditions have completed), and its result (the handle output
by the scheduled operation) becomes available when the operation completes. If there are any
dependencies scheduled on the operation, on completion the output handle is sent as input
to each of dependencies.

What this means is straightforward: in the example above `file()` outputs on completion
an open file handle which is fed to `truncate()` which on completion feeds the same input
handle to `write()` and so on. `close()` takes in the open file handle, closes it, and
outputs a null file handle on completion to `rmfile()` which doesn't use its input file
handle as it takes a path instead.

Note that [*errors do not propagate] except under limited circumstances. If `truncate()`
failed let's say due to lack of free space, no error state is propagated to `write()`
(though that would surely not succeed as async writing cannot go past the current
extent of a file). If `open()` failed, its output file handle would be null and therefore
every subsequent action would almost certainly also fail, but the sequence of scheduled
operations [*always] executes, regardless. In fact, in the present version of AFIO there
is no way whatsoever to cancel a scheduled operation.

Note how we gather exceptions from ops in the `when_all()` using an initialiser list.
Not performing a `when_all()` on an op is equivalent to not checking it for an error.
If you just want to wait for it to complete and not rethrow any exceptions encountered,
there is a `when_all(std::nothrow_t(), ...)` overload available.

[endsect] [/hello_world]

[section:file_concat A less toy example: Concatenating files]

The Hello World example didn't really demonstrate why using AFIO is any better than using
STL iostreams. The next example after this one will give a ["real world] and unfortunately
somewhat overwhelming in complexity example of how
AFIO can run rings around STL iostreams (complete with benchmarks!), so as an intermediate
__dash__ and hopefully not as overwhelming __dash__
step here is a simple file copy utility which can also concatenate multiple source files
into a destination file[footnote My thanks to Artur Laksberg, Dev Lead of the Microsoft
Visual C++ Parallel Libraries team, for suggesting this as a good demonstration of an
asynchronous file i/o library. I had been until then quite stumped for an intermediate
quick start example between the first and last examples.].

[filecopy_example]

This example consists of a function called `async_concatenate_files()` which will 
asynchronously append those file paths in ['sources] into the file path in ['dest], with
the `main()` function simply parsing arguments and printing progress every second.
I won't explain the example hugely __dash__ it's pretty straightforward, it simply
parallel reads all source files in `chunk_size` chunks, writing them to their appropriate
offset into the output file. It
is a very good example, incidentally, of why C++11 is like a whole new language over
C++98 even for simple systems programming tasks like this one.

The chances are that this file copying implementation would be little faster than a naive
implementation however (unless the source files are on different physical devices, in which
case you'd see maximum performance). Some might also argue that firing a consumer producer thread per source
file would be just as easy, albeit that output file position management across threads
might be slightly tricky.

Let us now look at two examples where the AFIO implementation blows
away a naive multithreaded example: a multiprocess safe log file and
finding which files in a directory tree contain a regular expression.

[endsect] [/file_concat]

[section:atomic_logging A multiprocess safe log file]

__triplegit__, the forthcoming reliable graph database store, uses a neat trick to avoid
having to use file locking during transaction commits, thus allowing multiple processes to
concurrently read and write the graph store without file locking. Each collection of hashes which form
the tree which makes up some given version of the graph is itself a hashed object in the content
addressable store, and to transact a new version of that graph one takes the following steps:

1. Atomically append a new 512 bit (64 byte) record to the log file for the graph. This record
contains the 256 bit hash (32 bytes) of the tree object representing the new version, two bytes of the record
index where we think this entry will append modulo 16 bits (2 bytes), the local version number we think this
ought to be (2 bytes), the current date and time (8 bytes), reserved fields (4 bytes) and a SipHash of the 48
bytes preceding the final 12 bytes of the record plus the 64 bytes of the last known good record, and
finally four bytes of Hamming (127, 120) coding for the preceding 60 bytes (480 bits) which enable
one bitrot per 15 bytes to be healed.

2. If more than one thread or process appends simultaneously, due to the atomicity only one will come
first. We can tell if we "won" by reading backwards from the end until we find our entry which if it
has the correct record index we know it is now the canonical new version. If it does not have the
correct index, we will have to recalculate a new transaction from the latest known good entry or
return an error to the end user as per their request.

3. It may now occur to you that surely the log file will grow forever! Luckily, modern filing systems
are ['extent based] which means you can ["punch holes] into file storage, and AFIO has a function
for that called __afio_zero__ which ostensibly zeros a region of a file, but will also deallocate
it if the filing system supports it. In our example below, we simply chop off the front of the log
file such that no more than 4Kb of data is actually stored on disc, even if the log file appears
to be many gigabytes or even terabytes in size!

(It isn't shown in the example code below, but the 8 byte hash is used to roll back the log to
a known good state in the event of sudden power loss which can be detected by spotting if the file
begins with zeros as for proper shutdown the dud entries in the log are removed, and the entries
compacted. One examines the index at the end of the file
and walks backwards for about 128Kb, healing any bit flips and checking the hashs. If any do not match
up, the entire index is parsed from the beginning up until the first entry with an incorrect hash, at which point the index
is truncated)

[endsect] [/atomic_logging]

[section:so_what What benefit does asynchronous file i/o bring me? A demonstration of AFIO's power]

So we can schedule file i/o operations asynchronously with AFIO: what's the benefit of doing that
instead of creating separate threads of execution and executing the file i/o there instead?

As a quick summary as we're about to prove our case, there are three main benefits to using AFIO
over doing it by hand:

# You don't have to worry about threading or race conditions or losing error state (as much). AFIO
does all that for you.
# On platforms which provide native asynchronous file i/o support and/or native scatter/gather
file i/o support, AFIO will use that
instead of issuing multiple filing system operations using a thread pool to achieve
concurrency. This can very significantly reduce the number of threads needed to keep your
storage device fully occupied __dash__ remember that ['queue depth] i.e. that metric you see all
over storage device benchmarks is the number of operations in flight at once, which implies
the number of threads needed. Most storage devices are IOPS limited due to SATA or SAS
connection latencies without introducing queue depth __dash__ in particular, modern SSDs cannot
achieve tens of thousands of IOPS range without substantial queue depths, which without
using a native asynchronous file i/o API means lots of threads.
# It's very, very easy to have AFIO turn off file system caching, readahead or buffering
on a case by case basis which makes writing scalable random synchronous file i/o applications
much easier.

What these three items mean is that writing scalable high-performance filing system code is much easier.
Let us take a real world comparative example, this being a simple STL iostreams, Boost Filesystem and OpenMP
based find regular expression in files implementation:

[find_in_files_iostreams]

This implementation is fairly straightforward: enumerate all files in all directories below
the current directory into a vector, fire off N threads to open each file, read it entirely
into memory, regex it for the pattern and if found print the file's path.

Let's now look at the AFIO implementation, and you should prepare yourself because it is far
more mind bendy due to the many nestings of callbacks needed (it may remind you of WinRT or .NET
code, everything is asynchronous callback):

[find_in_files_afio]

Here the `find_in_files` class is used to carry state across the callbacks __dash__ one could just
as equally bind the state into each callback's parameters instead via some sort of pointer to
a struct. But the difference in complexity is noticeable __dash__ you're probably now asking, why
choose this hideous complexity over the much clearer OpenMP and iostreams implementation[footnote
Indeed many ask the same when programming WinRT apps __dash__ Microsoft's insistance on never allowing
any call to block can make simple designs explode with complexities of nested callbacks.]?

Well, that's simple: do you want maximum file i/o performance? Here is a search for ["Niall] in a
Boost working directory which contained about 7Gb of data across ~35k files[footnote Test machine
was a 3.5Ghz Intel i7-3770K Microsoft Windows 8 x64 machine with Seagate ST3320620AS 7200rpm hard
drive. Note that AFIO has a native WinIOCP backend which leverages host asynchronous file i/o support.]:

[table:find_in_files_performance Find in files performance for traditional vs AFIO implementations
  [[][iostreams, single threaded][iostreams, OpenMP][Boost.AFIO][Boost.AFIO `file_flags::OSDirect`][Boost.AFIO `file_flags::OSMMap`[footnote The superiority of memory maps on Windows is because all buffered file i/o is done via memory copying to/from memory maps on Windows anyway, so eliminating that memory copy is huge.]]]
  [[Warm cache][812 Mb/sec][1810 Mb/sec][2663 Mb/sec][N/A][6512 Mb/sec]]
  [[][+0%][+123%][+228%][][+702%]]
  [[Cold cache[footnote File cache reset using [@http://technet.microsoft.com/en-us/sysinternals/ff700229.aspx]]][16 Mb/sec][[*8 Mb/sec]][15 Mb/sec][13.14 Mb/sec][24 Mb/sec]]
  [[][+0%][[*-50%]][-6%][-18%][+50%]]
]

Note how AFIO outperforms the OpenMP iostreams implementation by about 50% for a warm cache, with only
a 6% penalty for a cold cache over a single threaded implementation. Note the [*50%] penalty the OpenMP
iostreams implementation suffers for a cold cache __dash__ a naive multithreaded implementation causes
a lot of disc seeks. If you map a file into memory using `file_flags::OSMMap`, AFIO will `memcpy()` from
that map instead of reading __dash__ or of course you can use the map directly using the pointer returned
by `try_mapfile()`.

The eagle eyed amongst you will have noticed that the AFIO implementation looks hand tuned with a
special depth first algorithm balancing concurrency with seek locality __dash__ that's because I invested
two days into achieving maximum possible performance as a demonstration of AFIO's power (and to find
and fix some bottlenecks). Some might argue that this is therefore not a fair comparison to the OpenMP iostreams
implementation.

There are two parts to answering this argument. The first is that yes, the OpenMP iostreams search
algorithm is fairly stupid and simply tries to read as many files as there are CPUs, and those files could
be stored anywhere on the spinning disc. Because AFIO
can issue far more concurrent file open and read requests than OpenMP, it gives a lot more scope to
the filing system to optimise hard drive seeks and satisfy as many requests as is feasible __dash__
sufficiently so that with a cold cache, AFIO is a little slower than a single threaded iostreams
implementation where the filing system can spot the access pattern and prefetch quite effectively.
A completely valid solution to the OpenMP performance deficit would be to increase thread count dramatically.

The second part of answering that argument is this: AFIO's very flexible op chaining structure lets you
very easily twiddle with the execution dependency graph to achieve maximum possible performance by
balancing concurrency (too much or too little is slower, what you need is just the right balance)
and disc seeks (enumerations are best not done in parallel, it defeats any prefetching algorithm),
unlike the naive OpenMP implementation which is much harder to play around with. Don't get me wrong:
if you have plenty of time on your hands, you can implement a hand written and tuned find in files
implementation that is faster than AFIO's implementation, but it will have taken you a lot longer,
and the code will neither be as portable nor as maintainable.

[endsect] [/so_what]

[endsect] [/async_file_io]


[section:closure_engine Closure Execution Engines]

[def __msvc__ MSVC]
[def __stlport__ [@http://sourceforge.net/projects/stlport STLport]]

Because some file system operations are synchronous on most platforms (creating files,
calling `fsync()` etc.) __boost_afio__ internally needed an asynchronous, chainable, batch
executable, closure execution engine which it also makes available for public usage.
While this is a somewhat intimidating characterization, the core concept 
of such an engine is fairly straight forward. The following sections will attempt 
to clear up exactly what this title means, and to give the user a reference for how it 
can be used when compared to some better known implementations. If you are already 
familiar with what an asynchronous, chainable, batch executable, closure execution engine 
is you may wish to skip this section. 


[section The Basics]


First, it will be useful to introduce some core ideas before moving on to a more indepth 
look at closure execution engines. Let us begin with a simple definition of a closure. 
Wikipedia.org supplies a clear definition:


[:“In programming languages, a closure (also lexical closure or function closure) is a function or 
reference to a function together with a referencing environment—a table storing a reference to 
each of the non-local variables (also called free variables or upvalues) of that function.[1] 
A closure—unlike a plain function pointer—allows a function to access those non-local variables 
even when invoked outside of its immediate lexical scope.”] [footnote [@http://en.wikipedia.org/wiki/Closure_(computer_science)] Sussman and Steele. "Scheme: An interpreter for extended lambda calculus". "... a data structure containing a lambda expression, and an environment to be used when that lambda expression is applied to arguments." ([@http://en.wikisource.org/wiki/Page:Scheme_-_An_interpreter_for_extended_lambda_calculus.djvu/22 WikiSource])]


Basically, a closure can be thought of as an anonymous function object capable of capturing local
state for use outside of its lexical scope. In C++ a closure strongly implies the use of a lambda 
expression, though there are other ways to create closures. C++ is a very powerful and flexible 
language, and how a closure is created is of less importance than what it does. If you are 
unfamiliar with lambdas or closures in C++ we encourage you to do some research on the subjects 
before continuing, as a basic understanding of these core concepts will be important moving 
forward. 


Now that we have a basic idea of what a closure is, consider the role that a closure execution 
engine plays: to execute the closures supplied to it. How the engine works is largely driven by 
its design goals and performance criteria. A simplistic analogy is to consider a rocket engine 
and a combustion engine found in an automobile: both engines fill similar roles (vehicle 
propulsion) but are quite different in how they work and perform the basic task of vehicle 
locomotion, ie their design goals and performance criteria are completely different. A closure 
execution engine is a fairly general term, and implies little about the underlying implementation.


Earlier, we defined __boost_afio__ as a specialized type of closure execution engine: an asynchronous, 
chainable, batch executable, closure execution engine. Now that we have a basic understanding of 
what a closure execution engine is, understanding that it is asynchronous, chainable and batch 
executable aren't quite as daunting. 


*Asynchonous: closures can be completed in an asynchronous fashion, rather than with traditional 
sequential blocking behavior. Note that asynchronous does not necessarily imply concurrency. 
*Chainable: closures can be chained together, so that dependent operations can be scheduled 
for execution after the closures they depend on have been completed. 
*Batch Executable: A group of closures can be  scheduled for execution. 


So, __boost_afio__ can execute batches of sequential closures for asynchronous completion. What this
 means for users is that it is possible to break up a series of sequential tasks into a closures, and 
have __boost_afio__ schedule them for asynchronous completion while the main thread is free to complete 
other work. 

__boost_afio__ is primarily implemented as a threadpool that executes the closures fed to it based on
some internal logic. In essence __boost_afio__ creates a thread whose responsibility it is to schedule closures for execution within a threadpool. This allows user code to schedule some operations for future completion, and even to schedule a series of operations to be executed one after another, while leaving the main application free to complete other work while waiting for closures to execute. This ability to chain closures together makes it possible to schedule work to begin as soon as it is able, and avoids waiting for an entire set of operations to complete before any other tasks can begin. 

[section Examples]

Because __boost_afio__'s main focus is on file I/O, we've selected an example where the I/O operations are 
the source of blocking behavior. Take a simple example of reading in an array of integers from a 
file, and then preforming some calculations with the entries of the array. Traditionally, a user 
would be required to open the file and read the contents of the entire file before storing 
the data in an array, so that computations with that data can begin. With __boost_afio__ it is possible to
schedule the file to be opened, then for each integer to be read into memory, and have a calculation 
scheduled for immediate execution after the specific data it requires has finished being read into
memory. Finally, the file can be scheduled to be closed after all read operations have been completed.
This is a trivial example, but it serves to illustrate how closures can be used and scheduled to improve
performance. These benefits are not limited to file I/O, and improvements can be gained from using other
types closurse wisely. 


[h3 Traditonal example]

[closure_execution_traditional_example]

[h3 __boost_afio__ Example]

[closure_execution_afio_example]

While the example above is a bit contrived, it serves as a good example of the basic functionality
__boost_afio__ brings to the table. For a small array of only 10 integers, this is a bit of overkill,
but for a very large array, or a more complicated bit of code, the benefits may be quite impressive.
These basic examples show that the new code is still readable, concise, and clear, all while giving the 
user the benefit of asynchronous execution of discrete tasks. It is important to understand that not every 
set of tasks are readily translatable into a series of asynchronous operations, but for those that are 
__boost_afio__ provides a powerful tool for scheduling and executing them.

[endsect]
[endsect]

[section How __boost_afio__ Relates to Existing Closure Execution Engines]

Proposals N3562 and its predecessor N3378  outline an executor library based on internal 
implementations of similar facilities at Google and Microsoft. __boost_afio__ is based, in part, on some of 
the concepts and rationale from those papers. So how does __boost_afio__ relate to the new facilities 
proposed in those papers?


[h4 Abstract Executor]


`class executor` has an __boost_afio__ equivalent in `class async_file_io_dispatcher_base` (hence referred
to as “ `dispatcher` ” for brevity). The two classes are similar in that both are abstract base classes, 
and both are largely responsible for scheduling closures for execution. Technically `dispatcher` does not 
have an analog for `executor::add(std::function<void()>)` as the methods it uses to schedule closures for 
execution do not accept `std::function<void()>`. There is, however, a mechanism for adding closures for 
future execution: `completion()`, and its convenience wrapper `call()`. Both of these functions allow the 
user to schedule closures for future execution, though in slightly different ways. Additionally,
`dispatcher` provides several specialized versions of `executor::add()` in the form of specialized file 
operations (eg. `file, open, close, dir, rmdir`, etc.), which schedule these tasks for future execution. 

Because __boost_afio__ is designed to be chainable and batch executable, its functions tend to return 
futures, or vectors of futures, to obtain results from currently scheduled tasks, or chain a series of 
tasks together. While there is nothing in N3562 to forbidding such behavior, it doesn't explicitly call for 
it either.  


[h4 Executor Factory]


__boost_afio__ does provide an executor factory function `make_async_file_io_dispatcher()`, which is 
responsible for creating new dispatchers. Unlike the proposed factory functions in N3378, there is no 
distinction made between factories. This is similar to `singleton_inline_executor()` from N3562, except 
`make_async_file_io_dispatcher()` is not guaranteed to return a singleton. 


[h4 Scheduled Executors]


__boost_afio__ has no support for the scheduled_executor class. As a result it also does not support `add_at()` or `add_after()` functions. 

[h4 Default Executors]


__boost_afio__ has no notion of a default executor, and hence the functions `default_executor()` and `set_default_executor()` proposed in N3562 have no analog in __boost_afio__.


[h4 Concrete Executor classes]


These classes are the derived classes of the abstract base classes `executor` and `async_file_io_dispatcher_base`. N3562 proposes two such derived classes, a `loop_executor`, and a `serial_executor`. __boost_afio__ does not support either of these paradigms, but rather has two derived classes that handle different operating systems (Windows and POSIX) that are created through the factory function `make_async_file_io_dispatcher_base()`. 


__boost_afio__ does have access to a `loop_executor` analog, via Boost.ASIO. `boost::asio::io_service` can be thought of as a sort of `loop_executor`, and has many of the member functions called for in N3562. 


[h4 Threadpools]


__boost_afio__'s aysnc_file_io_dispatcher_base uses a threadpool rather than other executors to handle closures, and as a result is very similar to N3562's class thread_pool.  __boost_afio__ provides the thead_source and std_thread_pool classes to give  dispatcher similar functionality the N3562's class thread_pool. 


[h4 Synchronization Mechanisms]


__boost_afio__ provides several methods to synchronize results for asynchronous operations:

# barrier() : synchronize based on a group of ops completing. 
# when_all() : synchronize based on an entire group of ops completing. 
# when_any() : synchronize based on the first completion from a group of ops



[section Comparison of AFIO with other Closure Execution Engines]

The following tables illustrate how __boost_afio__ relates to some proposed closure execution engines. 
These proposals are related to the internal implementations of such systems at Google and Microsoft.
[@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3378.pdf N3378] was Google's initial proposal for such 
an engine, and its successor, [@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3562.pdf N3562] was a 
joint effort between Microsoft and Google. Anyone wishing to compare __boost_afio__ to an engine related to 
one of these proposals should find the following tables useful.


[table Comparison of N3378 with __boost_afio__ Equivalents
    [ [N3378 Feature] [__boost_afio__ Equivalent] ]
    [ [Abstract base class `executor`] [Abstract base class `async_file_io_dispatcher_base`] ]
    [ [`executor::add(std::function<void()>)`] [`async_file_io_dispatcher_base::completion(const async_io_op &, const std::pair<async_op_flags, std::function<async_file_io_dispatcher_base::completion_t>> &)`] ]
    [ [] [`async_file_io_dispatcher_base::call(const async_io_op &, std::function<R()> )`] ]    
    [ [`executor::try_add(std::function<void()>)`] [No Equivalent] ]
    [ [`executor::add_at(std::function<void()>)`] [No Equivalent] ]
    [ [`executor::try_add(std::function<void()>)`] [No Equivalent] ]
    [ [`executor::num_pending_closures()`] [`async_file_io_dispatcher_base::count()`] ]
    [ [`executor::default_executor()`] [No Equivalent] ]
    [ [`executor::set_default_executor(executor *)`] [No Equivalent] ]
    [ [`new_inline_executor()`] [No Equivalent] ]
    [ [`singleton_inline_executor()`] [No Equivalent] ]
    [ [`new_synchonized_inline_executor()`] [No Equivalent] ]
    [ [] [`make_async_file_io_dispatcher(thread_source &, file_flags, file_flags)`] ]
    [ [`class loop_executor`] [`boost::asio::io_service`] ]
    [ [`loop_executor::loop()`] [`boost::asio::io_service::work`] ]
    [ [`loop_executor::run_queued_closures()`] [`boost::asio::io_service::run()`] ]
    [ [`loop_executor::try_one_closure()`] [`boost::asio::io_service::run_one()`] ]
    [ [`loop_executor::make_loop_exit()`] [`boost::asio::io_service::stop()`] ]
    [ [`class serial_executor`] [No Equivalent] ]
    [ [`serial_executor::underlying_executor()`] [No Equivalent] ]
    [ [`class thread_pool`] [`std_thread_pool`] ]
    [ [`class thread_manager`] [No Equivalent] ]
    [ [`thread_manager::new_queue(managed_queue::options&)`] [No Equivalent] ]
    [ [`thread_manager::set_default)num_cpus(unsigned (*num_cpus)())`] [No Equivalent] ]
    [ [`class managed_queue`] [No Equivalent] ]
    [ [`managed_queue::queue_options()`] [No Equivalent] ]
    [ [`managed_queue::wait_until_complete()`] [`async_file_io_dispatcher_base::wait_all(iterator first, iterator last)`] ]
    [ [`struct managed_queue::options`] [No Equivalent] ]
    [ [`class thread_manager_policy`] [No Equivalent] ]
    [ [`thread_manager_policy::eval(state &, action *)`] [No Equivalent] ]
    [ [`default_thread_manager_policy(unsigned (*num_cpus)())`] [No Equivalent] ]
    [ [`struct thread_manager_policy::state`] [No Equivalent] ]
    [ [`struct thread_manager_policy::action`] [No Equivalent] ]

]

[table Comparison of N3562 with __boost_afio__ Equivalents
    [ [N3562 Feature] [__boost_afio__ Equivalent] ]
    [ [Abstract base class `executor`] [Abstract base class `async_file_io_dispatcher_base`] ]
    [ [`executor::add(std::function<void()>)`] [`async_file_io_dispatcher_base::completion(const async_io_op &, const std::pair<async_op_flags, std::function<async_file_io_dispatcher_base::completion_t>> &)`] ]
    [ [] [`async_file_io_dispatcher_base::call(const async_io_op &, std::function<R()> )`] ]     
    [ [`executor::num_pending_closures()`] [`async_file_io_dispatcher_base::count()`] ]
    [ [ class `scheduled_executor`] [No Equivalent] ]
    [ [`scheduled_executor::add_at(std::function<void()>)`] [No Equivalent] ]
    [ [`scheduled_executor::add_after(std::function<void()>)`] [No Equivalent] ] 
    [ [`executor::default_executor()`] [No Equivalent] ]
    [ [`executor::set_default_executor(executor *)`] [No Equivalent] ]    
    [ [`singleton_inline_executor()`] [No Equivalent] ]
    [ [] [`make_async_file_io_dispatcher(thread_source &, file_flags, file_flags)`] ]
    [ [`class loop_executor`] [`boost::asio::io_service`] ]
    [ [`loop_executor::loop()`] [`boost::asio::io_service::work`] ]
    [ [`loop_executor::run_queued_closures()`] [`boost::asio::io_service::run()`] ]
    [ [`loop_executor::try_one_closure()`] [`boost::asio::io_service::run_one()`] ]
    [ [`loop_executor::make_loop_exit()`] [`boost::asio::io_service::stop()`] ]
    [ [`class serial_executor`] [No Equivalent] ]
    [ [`serial_executor::underlying_executor()`] [No Equivalent] ]
    [ [`class thread_pool`] [`std_thread_pool`] ]
]
[endsect]

[section Other Useful Comparisons]
__boost_afio__ also incorporates some important concepts presented in 
[@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3558.pdf N3558]. If you're not familiar with that 
proposal we encourage you to read it, as that proposal presents some important concepts about asynchronous 
operations.  The following table outlines some of the proposed features from N3558 with their 
__boost_afio_equivalents


[table Comparison of N3558 with __boost_afio__ Equivalents
    [ [N3558 Feature] [__boost_afio__ Equivalent] ]
    [ [```
future<int> f1 = async([]() { return 123; });
future<string> f2 = f1.then([](future<int> f) {
    return f.get().to_string(); // here .get() won’t block
 });
        ```] 
      [```
auto dispatcher = boost::afio::make_async_file_io_dispatcher();
std::pair<future<int>, async_io_op> someop = dispatcher->call(boost::afio::async_io_op(), []() { 
    return 123; }); 
std::pair<future<string>, async_io_op> chainedfuture = dispatcher->call(someop.second, [](future<int> f){ 
    return f.get().to_string();}, someop.first);
        ```] ]
    [ [`future.unwrap()`] [No Equivalent] ]     
    [ [```
future<int> f1 = async([]() { return possibly_long_computation(); });
if(!f1.ready()) {
    //if not ready, attach a continuation and avoid a blocking wait
    fl.then([] (future<int> f2) {
        int v = f2.get();
        process_value(v);
     });
}
 //if ready, then no need to add continuation, process value right away
else {
    int v = f1.get();
    process_value(v);
}
        ```] 
      [```
auto dispatcher = boost::afio::make_async_file_io_dispatcher();
std::pair<future<int>, async_io_op> someop = dispatcher->call(boost::afio::async_io_op(), []() {
    return possibly_long_computation(); }); 
std::pair<future<int>, async_io_op> chainedfuture = dispatcher->call(someop.second, [](future<int> f2) { 
    return f2.get());}, someop.first);

int v=chainedfuture.first.get()
process_value(v);
        ```] ]
    [ [`when_any(iterator first, iterator last)`] [`when_any(iterator first, iterator last)`] ]
    [ [`when_all(iterator first, iterator last)`] [`when_all(iterator first, iterator last)`] ] 
    [ [`make_ready_future()`] [No Equivalent] ]
    
]
[endsect]
[endsect]
[endsect][/ end of Closure Execution Engines]
[endsect]
